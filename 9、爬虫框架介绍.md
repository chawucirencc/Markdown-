### `Scrapy` ###

#### 一、安装：

在配置好```Python3```的环境之后，可以在```cmd```或者编辑器的终端窗口里使用```pip3 list```查看已安装的```Python```模块。
**安装 ```scrapy``` 框架：**

Windows上：在```cmd```或者编辑器的终端窗口里使用```pip3 install scrapy```,等待提示即可完成。输入```scrapy -v```可以查看版本以及帮助命令。如图：

![](https://user-gold-cdn.xitu.io/2019/8/12/16c838105fb47c78?w=767&h=403&f=png&s=42280)
**使用```scrapy startproject projectname```可以创建爬虫项目，创建完成之后会出现如下图所示的文件结构：**

**注：红色方框所标记的文件夹不会被创建，它与```scrapy```无关，只是在```vscode```中的一些运行配置文件，可以忽略。**
![](https://user-gold-cdn.xitu.io/2019/8/12/16c8390e0d1ea2c8?w=261&h=222&f=png&s=12665)

在```mySpider```文件夹下面有一个```spiders```子文件夹和五个```.py```文件和一个```.cfg```文件。这里对结构做一个简要介绍。```spiders```文件夹下面是爬虫文件，```__init__.py```为此项目的模块文件，默认为空，暂时不用。```items.py```主要用来定义我们需要获取的数据。```middlewares.py```中间件文件，主要用来定义下载中间件和爬虫中间件的内容。```pipelines.py```处理获取到数据的文件。```settings.py```为项目配置文件，可以对项目进行一些自定义设置，比如开启或者关闭中间件，日志警告等级，启用或者关闭```pipelines```，以及其优先级的设置，是否开启```cookies```等。```scrapy.cfg```内的内容为下图：
![](https://user-gold-cdn.xitu.io/2019/8/12/16c83f2354e04d6a?w=887&h=368&f=png&s=51632)
默认为本项目的```settings.py```，项目为创建的项目名称。
使用```scrapy genspider spidername domain[starturl]```可以创建一个爬虫，使用命令之后如图：

![](https://user-gold-cdn.xitu.io/2019/8/12/16c848fc77e36f4f?w=936&h=134&f=png&s=14735)

使用```scrapy crawl spidername```可以启动爬虫项目。

#### 二、数据流向：
**关于数据流向整体大致分为5个部分:**
**1、**```Scheduler```：常称为调度器，存放``` Requests```对象，其中包括``` url```地址、``` headers```、``` post```数据，``` cookies```、代理等等。```Scheduler```会将``` Requests```对象通过``` Engine```发送到``` Downloader```模块进行处理。
**2、**``` Downloader```：常称为下载器，此模块在获得``` Requests```对象之后会对服务器发送请求，并且得到``` Response```，随后将``` Response```通过``` Engine```发送给``` Spiders```模块。
**3、**``` Spiders```：此模块用于提取所需要的数据，可以分为两个部分。第一部分，提取``` url```，``` headers```，``` cookis```，``` post```等数据组装成``` Requests```对象通过``` Engine```发给调度器。第二部分，将提取到有用的数据通过``` Engine```传递到``` Item Pipeline```模块。
**4、**``` Item Pipeline```：将爬取到的数据列入一个队列，另外是对爬取到的数据进行保存和其他的一些操作。
**5、**```Scrapy Engine```:为整个框架的调度，传递各个模块之间的数据，使得其他4个模块相互独立。除过``` Item Pipeline```。每个模块会首先将数据传给```Scrapy Engine```，然后由```Scrapy Engine```传递给下一个模块。大致的数据流如下图:

![](https://user-gold-cdn.xitu.io/2019/8/10/16c798907dcb6e7c?w=1400&h=940&f=png&s=53978)
在```Downloader```和``` Engine```之间存在一个```Downloader Middlewares```称为下载中间件，可以对自己的```Requests```请求作出一些修改，比如设置代理```IP```，修改```UA```等。在这里```Requests```和```Response```都是经过这个组件的。

在``` Spiders```和``` Engine```之间存在着一个```Spider Middlewares```称为爬虫中间件，处理来自```Downloader```的```Response```，可以对组成的```Requests```对象进行一些过滤操作。在这里```Spider Middlewares```不对想要得到的数据进行处理，因为有专门的```Item Pipeline```模块对想要得到的数据进行处理。

#### 三、各个模块的介绍：
1、关于```Scrapy Engine```模块，这一部分在项目建立的时候其实已经实现好了，不用我们再去手动编写，对于数据在各个模块之间的流动，```Scrapy Engine```会自行进行调度。</br>
2、调度器主要是处理来处理建立爬虫开始的```url```地址和经过组装之后的```Requests```对象，将其列成一个队列，有序的送给```Scrapy Engine```交给下一个模块。在传递给下载器之前会先经过下载中间件，如果开启了```Downloader Middlewares```组件的话，对象会先经过这个组件进行处理，做出相应的自定义修改，然后给下载器想服务器发送请求。</br>
3、下载器想服务器发送请求之后，服务器会给我们一个响应，这个响应可以通过```response.text```查看，返回的是一个网页的源代码（Ajax请求的方法可能另有不同），然后可以使用正则或者```Xpath```选择器对需要的数据进行提取。随后将响应结果交还给```Engine```，再由```Engine```交给```spider```。
4、在交给```spider```之前会有一个爬虫中间件，默认是关闭状态的。一般不用，而且在中间件中内置了一些方法，可以在做出相应操作的时候对方法进行调用。在```middlwares.py```文件中有相应的模板。经过中间件之后就到了```spider```模块了，这个地方是需要我们编写的，因为对于每次数据的获取都可能不太一样，所以要做出相应的改变。主要就是在我们新建的爬虫文件下的```parse```方法下进行处理得到数据。自己可以新建方法，但是这个方法不能被改名，而且是通过这个方法来将```item```数据传给```pipeline```的。
5、```pipeline```模块主要是对拿到的数据进行处理，而在```settings.py```文件中，```pipeline```模块是默认关闭的，所以需要手动开启。取消掉```pipeline```的注释即可。```pipeline```里的默认内容如下图：
![](https://user-gold-cdn.xitu.io/2019/8/16/16c99795fd228d2d?w=1132&h=386&f=png&s=45312)
而在```settings.py```文件中有```ITEM_PIPELINES```这个项目，并且是个字典，键为一个```pipeline```类，意味着在管道处理模块可以设置多个```pipeline```来处理数据，一方面是处理来自不同爬虫文件的数据，可以通过爬虫类的```name```属性来判断是哪个爬虫，以此来做出相应的数据处理。另一方面就是对于一个爬虫来说可以建立多个```pipeline```来处理数据，假设一个```pipeline```对数据进行处理，另外一个```pipeline```对数据进行保存工作。而这个字典的值就是当前```pipeline```的优先级，数字越小说明优先级越高，就越早执行。

两个中间件暂时不用，用到的时候再解释。

#### 四、关于所创建的爬虫的结构和方法：
在使用```scrapy genspider spidername```之后，会在```spider```目录下创建一个```spidername```的```py```文件，这个文件就是需要编写爬虫的地方，在创建好了爬虫文件之后如图：
![](https://user-gold-cdn.xitu.io/2019/8/15/16c95e3e3046c6e4?w=1088&h=351&f=png&s=41126)
这是一个```DtyySpider```类，下面有三个属性和一个方法，这个```parse```方法就是处理来自引擎的```response```。类的```name```属性为这个爬虫的名称，之后可以使用```scrapy crawl dytt```可以启动当前爬虫，当然这个属性还可以用来判断是哪个爬虫，可以在一个框架下面创建多个不同的爬虫。通过```name```属性来判断是哪个爬虫，然后可以在```pipeline```里面对不同的爬虫做相应的处理。

这里建议使用```scrapy shell url```进行调试，这里可以调试自己的正则、```CSS```、```Xpath```等语句是否正确，查看是否可以准确的提取到数据。然后再运行爬虫即可，这样做的好处是，在没有配置```IP```池的情况下，可以不用对网址进行请求，防止请求多次之后被网站禁封```IP```。

#### **五、这里先对我们创建的爬虫文件进行讲解：**
之前粗略的讲过这个类里面的属性和方法，这里详细讲解一下。
爬虫如何创建以及创建之后默认内容是什么上面已经说了。首先就是```DyttSpider```这个类，继承的是```scrapy.Spider```这个类，有兴趣的可以看看源码，这里不再多说。```allowed_domains```属性就是这个爬虫可以爬取的域名范围。```start_urls```就是这个爬虫开始的```url```地址。下面的```parse```方法就是对返回结果进行处理的函数，而且返回结果保存在```response```里面，我们可以使用正则、```Xpath```、```CSS```选择器等方法提取到我们想要的值。
***这里使用一个我自己做过的例子来讲解：*** 
我这里是对电影天堂电影信息的爬取，大约200余页的电影信息。代码如图：

![](https://user-gold-cdn.xitu.io/2019/8/22/16cb4f358e19f705?w=1658&h=935&f=png&s=228375)



对于非Ajax请求的网址，```reponse```可以返回我们在网页上看到的内容，通过一个按钮会跳转到另外一个```html```页面，同样的还是可以在源代码中看见网页上的内容。

***然而对于Ajax形式的请求，在网页源代码处不能直接看到，是通过```js```进行异步加载的，意思就是在浏览器地址栏的那个网址不是我们需要请求的网址，真正的网址需要在开发者工具通过抓包找到，然后对真正的```url```地址进行请求。***

这个时候对于新找到的```url```可能为```get```请求，也可能为```post```请求，要看清楚，然后再进行操作。


将持续更新......

### `pyspider` ###



待补充...