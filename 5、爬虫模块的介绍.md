模块的介绍：

---

 Python2 里常用```urllib``` 和 ```urllib2``` ，Python3 里常用 ```reuquests``` 模块。

logging模块可以将标准输出到日志文件保存起来。

### 1、请求模块requests： ###

参考文档： https://requests.kennethreitz.org//zh_CN/latest/user/quickstart.html 

```reuquests``` 应该算是对 ```urllib``` 和 ```urllib2 ``` 的扩展，使用起来更加方便，并且可以自动的把返回的信息进行解码。```get``` 请求用法：

通常会加上错误处理：

```python
url = "https://www.baidu.com/"
res = requests.get(url)
print(res.status_code)
res.encoding = 'utf-8'
print(res.text)
```

```post``` 请求用法：

```python
url = "XXXXXXXXXXXXXX"
data = {
    'key1':'value1',
    'key2':'value2'
}
res = requests.post(url, data)
print(res.status_code)
res.encoding = 'utf-8'
print(res.text)
```

在请求参数内添加```headers``` 参数，可以设置 IP 池和随机的 UA。

关于 Headers 中的一些信息：

*   Request URL：表示这个网页的请求地址。
*   Request Method：请求方法，大部分为 get 和 post 两种。
*   Status Code：状态码，正常的话会是200。
*   User-Agent：浏览器标识。
*   Cookies：登录信息的存放的地方。

### 2、网页解析模块： ###

##### `BeautifulSoup` #####

```BeautifulSoup``` 参考文档： https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/ 

从模块加载`BeautifulSoup`

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, "html.parse") # 解析方法可以选择其他更高性能的lxml、
```

在 `BeautifulSoup` 中使用的是 CSS 选择器。关于选择器的具体可以看选择器 MD 文件。

待补充....

##### `lxml` #####

```python
from lxml import etree
page = etree.HTML(response.text) # 返回的page是一个节点(Element)，使用选择器选择节点中的数据。
result = page.xpath(".//div[@class='test']//p//text")
# result节点得到所有class="test"的div节点下p节点的所有文本。返回结果是个list。
```

上面是解析html的方法。

还有其他的一些用法。

##### `pyquery` #####

```python

```



##### 正则表达式 #####



### 3、提取数据 ###

##### `Xpath`选择器 #####



##### CSS选择器 #####



##### 正则表达式 #####



爬虫框架可以参考另一份文件！











