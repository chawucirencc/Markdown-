### 一、关于```Xpath```选择器：

```
/：从根节点开始选取
//：从匹配选择的当前节点选择文档中的节点，不考虑位置
. ：选取当前节点
.. ：选取当前节点的父节点
@ ：选取属性
```
需要注意的是，在节点选取的过程中下标识从1开始的，而不是0.
在网页中，从```div```节点开始选取，```//div[@class=classname]```，由此选择属性名为```classname```的所有```div```节点。在爬虫获取数据的时候可以先将包裹所需数据的节点先生成一个列表。列表形如：

```[<Selector xpath="//div[@class='classname']//ul//table" data='<table width="100%" border="0" cellspaci'>, ..., <Selector xpath="//div[@class='classname']//ul//table" data='<table width="100%" border="0" cellspaci'>]```，每个元素就是一个选择器，然后对这个列表进行遍历，提取出每个选择器中的数据。
```
url = 'https://www.baidu.com/'
response = requests.get(url)
response.encoding = 'utf-8'
res = etree.HTML(response.text)
res_tree = res.xpath(".//a[@class='mnav']//text()")
```
通过 text() 可以提取出标签中的文本。

```python
def get_page_xpath():
    """
    获取网页响应......
    使用 Xpath 获取结果
    """
    try:
        url = 'https://www.baidu.com/'
        response = requests.get(url)
        assert response.status_code == 200
        # print(response.status_code)
        response.encoding = 'utf-8'
        res = etree.HTML(response.text)
        res_tree = res.xpath(".//a[@class='mnav']//text()")
        result = re.sub("string", "", res_tree)
        print(result)
    except Exception as e:
        print(e)
```

有时候从`xpath` 里面提取出来的文本有特殊符号，最后还需要使用正则表达式替换这些特殊符号。

----
二、```BeautifulSoup```中的选择器。
```python
from bs4 import BeautifulSoup
```
加载网页解析包。在 ```BeautifulSoup``` 中的选择器模式为 CSS 选择器。 
1、关于 select 选择。

```python
url = 'https://www.baidu.com/'
response = requests.get(url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'lxml')
# print(soup.prettify())
result = soup.select('.mnav')
for i in result:
    print(i.text)
```
```
. 代表class，
# 代表id，
```
组合查找： ```print(soup.select('p #link1') ``` 意思为，查找 p 标签中 ```id = link1``` 的标签。 
2、关于 find_all 选择。
```result = soup.find_all('a', attrs={'class': 'mnav'})``` 查询 a 标签中 ```class = mnav``` 的标签。查询的结果是一个列表，可以通过遍历 ```.texe``` 得到标签中的文本。

```python
def get_page_beautiful():
    """
    使用 CSS 选择器提取数据。
    """
    try:
        url = 'https://www.baidu.com/'
        response = requests.get(url)
        assert response.status_code == 200
        # print(response.status_code)
        response.encoding = 'utf-8'
        res = BeautifulSoup(response.text, 'lxml')
        # print(res.prettify())
        result = res.select("a[class=mnav]") # 第一种写法，性能相对来讲较好。
        result = res.select(".mnav") # 第二种写法。
        result = res.findAll('a', attrs ={'class':'mnav'}) # 第三种写法。
        for i in result:
            print(i.string) # 此处可以在外部写一个处理函数，将标签列表中的文本提取出来。
    except Exception as e:
        print(e)
```



三、正则表达式、
将网页解析后，```response.text``` 返回的结果是一个字符串，当然也可以通过正则表达式来获取网页里的内容。
第一种就是找到需要提取的数据的部分，看是否所有的的格式都相同，可以用```(.*?)``` 来提取数据。
一般来讲，提取到的数据格式也并不是完美的，也是需要正则表达式来处理最终的结果。

更多的情况下是使用正则来处理数据格式，去除其中的特殊字符和空格等。

